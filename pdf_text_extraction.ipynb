{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf24e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"Your_GPT_key_here\"\n",
    "pdf_directory = \"Your_pdf_directory\"\n",
    "# Example on windows: r\"C:\\Users\\Name\\Desktop\\documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd582bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dokument import Dokument\n",
    "import os\n",
    "import fitz \n",
    "\n",
    "# For getting DOI from file name, modify this to the naming convention of your prepared PDF folder\n",
    "def get_doi_from_filename(filename):\n",
    "    return \"10.1145/\" + os.path.splitext(filename)[0]\n",
    "\n",
    "# Initialize the list to hold Dokument objects\n",
    "dokument_list = []\n",
    "# File counter to keep track of number of files processed\n",
    "file_counter = 0\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_counter += 1\n",
    "        file_path = os.path.join(pdf_directory, filename)\n",
    "        doi = get_doi_from_filename(filename)\n",
    "\n",
    "        with fitz.open(file_path) as doc:\n",
    "            raw_data = \"\"\n",
    "            for page_num in range(doc.page_count):\n",
    "                page = doc[page_num]\n",
    "                # Extract raw text from the document\n",
    "                raw_data += page.get_text()\n",
    "        # Debug lines\n",
    "        print(f\"Processing PDF file number {file_counter}: {filename}\")\n",
    "        print(\"DOI:\", doi)\n",
    "        print(\"Extracted text:\", raw_data[:100])  # Print first 100 characters for debug\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Create a Dokument object with DOI and raw data attributes and add it to the list\n",
    "        dokument = Dokument(DOI=doi, raw_data=raw_data)\n",
    "        dokument_list.append(dokument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of documents for debugging\n",
    "print(len(dokument_list), \"documents with raw data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(dokument_list, model):\n",
    "    long_documents_found = False\n",
    "    print(\"Documents with more than 127,000 tokens:\")\n",
    "    for i, document in enumerate(dokument_list):\n",
    "        # Count the number of tokens in the document with the language model\n",
    "        num_tokens = model.get_num_tokens(document.raw_data)\n",
    "        document.token_count = num_tokens\n",
    "        if num_tokens > 127000:\n",
    "            long_documents_found = True\n",
    "            # Debug message for documents exceeding the token limit\n",
    "            print(f\"Document {i + 1}: DOI = {document.DOI}, Tokens = {num_tokens}\")\n",
    "    if not long_documents_found:\n",
    "        print(\"No documents have more than 127,000 tokens.\")\n",
    "    # Print information about the token limit of the GPT-4 model\n",
    "    print(\"\"\"\"\\ngpt-4-0125-preview model only supports up to 128,000 tokens. \n",
    "    Documents with raw data exceeding 127,000 will not be added to document list to be used for research.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to define LLM model to count token\n",
    "from langchain_openai import ChatOpenAI\n",
    "gpt4_model = ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\")\n",
    "count_tokens(dokument_list, gpt4_model)\n",
    "\n",
    "dokument_list = [dokument for dokument in dokument_list if int(dokument.token_count) <= 127000]\n",
    "# debug\n",
    "print(\"After filtering out documents with more than 127000 tokens,\",len(dokument_list),\"documents remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dokument_list with pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"dokument_list.pkl\", \"wb\") as file:\n",
    "    pickle.dump(dokument_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874fb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample_env",
   "language": "python",
   "name": "sample_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
